{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import skimage.io\n",
    "import skimage.transform\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import imshow\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Local imports\n",
    "from network_helpers import load_image\n",
    "from style_helpers import gramian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VGG_NETWORK_NAME = \"vgg\"\n",
    "\n",
    "content_image = tf.placeholder(\"float\", [1, 224, 224, 3])\n",
    "style_image = tf.placeholder(\"float\", [1, 224, 224, 3])\n",
    "synthesized_image = tf.Variable(tf.random_uniform([1, 224, 224, 3]))\n",
    "network_input = tf.concat(0, [content_image, style_image, synthesized_image])\n",
    "with open(\"models/vgg16.tfmodel\", mode='rb') as f:\n",
    "            file_content = f.read()\n",
    "graph_def = tf.GraphDef()\n",
    "graph_def.ParseFromString(file_content)\n",
    "tf.import_graph_def(graph_def, input_map={\"images\": network_input}, name=VGG_NETWORK_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gramian_for_layer(layer):\n",
    "    \"\"\"\n",
    "    Returns a matrix of cross-correlations between the activations of convolutional channels in a given layer.\n",
    "    \"\"\"\n",
    "    activations = tf.get_default_graph().get_tensor_by_name(\"{0}/conv{1}_1/Relu:0\".format(VGG_NETWORK_NAME, layer))\n",
    "\n",
    "    # Reshape from (batch, width, height, channels) to (batch, channels, width, height)\n",
    "    shuffled_activations = tf.transpose(activations, perm=[0, 3, 1, 2])\n",
    "    return gramian(shuffled_activations)\n",
    "\n",
    "layers = [i for i in range(1, 6)]\n",
    "activations = [tf.get_default_graph().get_tensor_by_name(\"{0}/conv{1}_1/Relu:0\".format(VGG_NETWORK_NAME, i)) for i in layers]\n",
    "gramians = [gramian_for_layer(x) for x in layers]\n",
    "# Slices are for style and synth image\n",
    "gramian_diffs = [tf.sub(tf.slice(g, [1,0,0], [1,-1,-1]), tf.slice(g, [2,0,0], [1,-1,-1])) for g in gramians]\n",
    "Ns = [g.get_shape().as_list()[2] for g in gramians]\n",
    "Ms = [a.get_shape().as_list()[1] * a.get_shape().as_list()[2] for a in activations]\n",
    "scaled_diffs = [tf.square(g) for g in gramian_diffs]\n",
    "style_loss = tf.div(tf.add_n([tf.div(tf.reduce_sum(x), 4*(N**2)*(M**2)) for x, N, M in zip(scaled_diffs, Ns, Ms)]), len(layers))\n",
    "\n",
    "activation_diffs = [tf.sub(tf.slice(a, [0,0,0,0], [1,-1,-1,-1]), tf.slice(a, [2,0,0,0], [1,-1,-1,-1])) for a in activations]\n",
    "content_loss = tf.div(tf.add_n([tf.reduce_sum(tf.square(a)) for a in activation_diffs]), 2.0)\n",
    "\n",
    "alpha = 0.001\n",
    "beta = 1.0\n",
    "combined_loss = tf.add(tf.mul(beta, style_loss), tf.mul(alpha, content_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.005, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')\n",
    "train_step = optimizer.minimize(combined_loss)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "style_image_input = load_image(\"img/style.jpg\").reshape((1, 224, 224, 3))\n",
    "content_image_input = load_image(\"img/content.jpg\").reshape((1, 224, 224, 3))\n",
    "print(\"Loss\", sess.run(combined_loss, feed_dict={content_image: content_image_input, style_image: style_image_input}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(1,200):\n",
    "    train_step.run(feed_dict={content_image: content_image_input, style_image: style_image_input})\n",
    "    stylized_image = synthesized_image.eval().reshape((224, 224, 3))\n",
    "    img_fname = 'img/out_%04i.jpg' % i\n",
    "    skimage.io.imsave(img_fname, np.clip(stylized_image * 255, 0, 255).astype('uint8'))\n",
    "    output_str = \"Loss for step {0}: {1}\".format(i, sess.run(combined_loss, feed_dict={content_image: content_image_input, style_image: style_image_input}))\n",
    "    display.clear_output(wait=False)\n",
    "    print(output_str)\n",
    "    display.display(display.Image(img_fname))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
